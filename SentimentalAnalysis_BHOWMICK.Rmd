---
title: "DS745_SentimentalAnalysis_BHOWMICK"
author: "Gautam"
date: "December 9, 2018"
output: word_document
---

```{r setup, include=FALSE}
setwd(getwd())
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
# Install and load the following packages if not installed
#install.packages("twitteR")
#install.packages("openssl")
#install.packages("httpuv")
#install.packages("RCurl")
#install.packages("tm")
#install.packages("wordcloud")
#install.packages("SnowballC")
#install.xts("xts")
#source("http://bioconductor.org/biocLite.R")
#biocLite("Rgraphviz")
#install.package("igraph")
#install("qdap")
```

# load all the libraries required for analysis

```{r, warning=FALSE}
library(openssl)
library(xts)
library(httpuv)
library(ggplot2) 
library(twitteR)
library(RCurl)
library(tm)
library(wordcloud)
library(SnowballC)
library(Rgraphviz)
library(topicmodels)
library(plyr)
library(dplyr)
library(stringr)
```

# Application keys

```{r}

consumer_key<-'Pnn9DsMpxfM0MSKznlA3RPgsM'
consumer_secret<-'AOczWZ3fjePKzIuxRiObwUvpQAYUL1n0fP2dCb1Xjkn0oUOAvU'
access_token<-'784558015030632448-7oWodTyR7IcRlzefr3ZTz6aB0vgQkHl'
access_secret<-'3hqG2EOnhrm5Xy8eWMSB17dLSvK1oM3nzDb6AnJDTKfJ9'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```

# Collecting tweets

```{r}
# ****Used first time to extract tweets***Comment Code
findfd= "#gunviolence"
number= 30000
bd_tweets = searchTwitter(findfd,  n=number, lang="en")
tweets.df <- twListToDF(bd_tweets) # convert tweets to a data frame
```

# Time Series of tweets

```{r}
##The xts function creates a timeline from a vector of values and a vector of timestamps.
#If we know how many tweets we have, we can just create a simple list or vector containing that number of 1s

ts=xts(rep(1,times=nrow(tweets.df)),tweets.df$created)

#We can now do some handy number crunching on the timeseries, such as applying a formula to values contained with day, week, month, quarter or year time bins.
#So for example, if we sum the unit values in daily bin, we can get a count of the number of tweets per day
ts.sum=apply.daily(ts,sum) 
#also apply. weekly, monthly, quarterly, yearly

#If for any resason we need to turn the timeseries into a dataframe, we can:
ts.sum.df=data.frame(date=index(ts.sum), coredata(ts.sum))

colnames(ts.sum.df)=c('date','sum')

#We can then use ggplot to plot the timeseries...
ggplot(ts.sum.df)+geom_line(aes(x=date,y=sum))
```

# Chart a count of the number of tweets by day, week, or hour

```{r}
tweets.df <- tweets.df[ -c(2:4, 6:16) ]
# Create a corpus
bd_corpus = VCorpus(VectorSource(tweets.df$text))
#label a tweet with the month number
tweets.df$month=sapply(tweets.df$created, function(x) {p=as.POSIXlt(x);p$mon})
#label a tweet with the hour
tweets.df$hour=sapply(tweets.df$created, function(x) {p=as.POSIXlt(x);p$hour})
#label a tweet with a number corresponding to the day of the week
tweets.df$wday=sapply(tweets.df$created, function(x) {p=as.POSIXlt(x);p$wday})

ggplot(tweets.df)+geom_jitter(aes(x=wday,y=hour))
```

# Clean the corpus by removing punctuation, numbers, and white spaces

```{r}
bd_clean  <- tm_map(bd_corpus, tolower)
bd_clean  <- tm_map(bd_clean, removePunctuation)
bd_clean  <- tm_map(bd_clean, removeNumbers)
#This means that all the words are converted to their stem (e.g. learing->learn)
bd_clean  <- tm_map(bd_clean, stemDocument)
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
#Stopwords are commonly used words in the English language such as I, me etc
bd_clean  <- tm_map(bd_clean, removeWords, myStopwords)
bd_clean  <- tm_map(bd_clean, PlainTextDocument)
bd_clean  <- tm_map(bd_clean, stripWhitespace)

```

# 25 most frequest words in word cloud

```{r}
# 25 most frequest words in word cloud
dtm <- DocumentTermMatrix(bd_clean)
tdm <- TermDocumentMatrix(bd_clean) 
freq.terms <- findFreqTerms(tdm, lowfreq = 700)
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 700)
df <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
  xlab("Terms") + ylab("Count") + coord_flip()
mat_rix <- as.matrix(dtm)
vec_tor <- sort(colSums(mat_rix),decreasing=TRUE)
words <- names(vec_tor)
data <- data.frame(word=words, freq=vec_tor)
# display 25 most frequent words
head(data,25)

```

# Word Association

```{r}
findAssocs(tdm, "kill", 0.2)
findAssocs(tdm, "gun", 0.2)

```

# Word Clustering

```{r}
# remove sparse terms
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward.D")
plot(fit)
rect.hclust(fit, k = 6)
```

# wordcloud and network diagram

```{r, warning=FALSE}
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")[-(1:4)]
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,
          random.order = F, colors = pal)
plot(tdm, term = freq.terms, corThreshold = 0.1)

```

# Topic Modelling

```{r}
lda <- topicmodels::LDA(dtm, k = 8) # find 8 topics
term <- terms(lda, 7) # first 7 terms of every topic
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topics <- topics(lda) # 1st topic identified for every document (tweet)
topics <- data.frame(date=as.Date(tweets.df$created), topic=topics)
ggplot(topics, aes(date, fill = term[topic])) + geom_density(alpha = 0.5, position = "stack")

```

# scan the positive word file and negative word file
```{r}

pos = scan('positive-word-list.txt', what='character', comment.char=';')
neg= scan('negative-word-list.txt', what='character', comment.char=';')

```

# returnpscore for counting the positive matching words.

```{r}

returnpscore=function(tweets) {
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', tweets)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    # and convert to lower case:
    sentence = tolower(sentence)
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    pos.match=match(words,pos)
    pos.match=!is.na(pos.match)
    pos.score=sum(pos.match)
    return(pos.score)
}
#Next we apply this function to the tweetclean list
positive.score=lapply(bd_clean,function(x) returnpscore(x))
#Next we define a loop to count the total number of positive words present in the tweets
pcount=0
for (i in 1:length(positive.score)) {
  pcount=pcount+positive.score[[i]]
}
pcount
```

# returnnscore for counting the positive matching words.

```{r}

returnnscore=function(tweets) {
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', tweets)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    # and convert to lower case:
    sentence = tolower(sentence)
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    neg.match=match(words,neg)
    neg.match=!is.na(neg.match)
    neg.score=sum(neg.match)
    return(neg.score)
}
#Next we apply this function to the tweetclean list
negative.score=lapply(bd_clean,function(x) returnnscore(x))
#Next we define a loop to count the total number of positive words present in the tweets
ncount=0
for (i in 1:length(negative.score)) {
  ncount=ncount+negative.score[[i]]
}
ncount
```

# The following code retrieves the positive matching words.

```{r}

poswords=function(tweets){
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', tweets)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    # and convert to lower case:
    sentence = tolower(sentence)
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    pmatch=match(words,pos)
    posw=pos[pmatch]
    posw=posw[!is.na(posw)]
    return(posw)
  }

```

#The code below creates matches of positive words

```{r}
pdatamart=data.frame()
for (t in bd_clean) {
  pdatamart=c(poswords(t),pdatamart)
}
pwords=unlist(pdatamart)
dpwords=data.frame(table(pwords))
dpwords=dpwords%>%
  mutate(pwords=as.character(pwords))%>%
  filter(Freq>15)

ggplot(dpwords,aes(pwords,Freq))+geom_bar(stat="identity",fill="lightblue")+theme_bw()+
  geom_text(aes(pwords,Freq,label=Freq),size=4)+
  labs(x="Major Positive Words", y="Frequency of Occurence",title=paste("Major Positive Words and Occurence in",findfd,"twitter feeds, n =",number))+
  geom_text(aes(1,200,label=paste("Total Positive Words :",pcount)),size=4,hjust=0)+theme(axis.text.x=element_text(angle=45))

wordcloud(words = dpwords$pwords, freq = dpwords$Freq, min.freq = 1,
          max.words=150, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

# The following code retrieves the negative matching words.

```{r}

negwords=function(tweets){
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', tweets)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    # and convert to lower case:
    sentence = tolower(sentence)
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    nmatch=match(words,neg)
    nosw=neg[nmatch]
    nosw=nosw[!is.na(nosw)]
    return(nosw)
  }

```

#The code below creates matches of negative words

```{r}
ndatamart=data.frame()
for (t in  bd_clean) {
  ndatamart=c(negwords(t),ndatamart)
}
nwords=unlist(ndatamart)
dnwords=data.frame(table(nwords))
dnwords=dnwords%>%
  mutate(nwords=as.character(nwords))%>%
  filter(Freq>20)

ggplot(dnwords,aes(nwords,Freq))+geom_bar(stat="identity",fill="lightblue")+theme_bw()+
  geom_text(aes(nwords,Freq,label=Freq),size=4)+
  labs(x="Major Negative Words", y="Frequency of Occurence",title=paste("Major Negative Words and Occurence in",findfd,"twitter feeds, n =",number))+
  geom_text(aes(0,9000,label=paste("Total Negative Words :",ncount)),size=4,hjust=0)+theme(axis.text.x=element_text(angle=45))

wordcloud(words = dnwords$nwords, freq = dnwords$Freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

#finally, we convert the matrix to a data frame, filter for Minimum frequency > 700 and plot using ggplot2

```{r}
# #removing sparse terms
dtms=removeSparseTerms(dtm,.99)
freq=sort(colSums(as.matrix(dtm)),decreasing=TRUE)
#get some more frequent terms
findFreqTerms(dtm,lowfreq=100)
wf=data.frame(word=names(freq),freq=freq)
wfh=wf%>%
  filter(freq>=700,!word==tolower(findfd))
ggplot(wfh,aes(word,freq))+geom_bar(stat="identity",fill='lightblue')+theme_bw()+
  theme(axis.text.x=element_text(angle=45,hjust=1))+
  geom_text(aes(word,freq,label=freq),size=4)+labs(x="High Frequency Words ",y="Number of Occurences", title=paste("High Frequency Words and Occurence in",findfd," twitter feeds, n =",number))+
  geom_text(aes(1,max(freq)-100,label=paste("Positive Words:",pcount,"\n","Negative Words:",ncount,"\n")),size=5, hjust=0)

```

# sentiment score function

```{r}
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
# we got a vector of sentences. plyr will handle a list # or a vector as an "l" for us # we want a   simple array ("a") of scores back, so we use # "l" + "a" + "ply" = "laply":
    scores = laply(sentences, function(sentence, pos.words, neg.words) {
    # clean up sentences with R's regex-driven global substitute, gsub():
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    # and convert to lower case:
    sentence = tolower(sentence)
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    # compare our words to the dictionaries of positive & negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    # match() returns the position of the matched term or NA # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    return(score)
    }, pos.words, neg.words, .progress=.progress )
    scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
  
}

```

# run the sentiment score function on the clean_text column and export the file

```{r}
scoreBOW = score.sentiment(tweets.df$text, pos, neg)
tweets.df$scoreBOW = scoreBOW$score

```

# create a new column sentiment, score >0 as positive, score=0 as neutral and score<0 as neutral

```{r}

tweets.df$sentimentBOW= ifelse(tweets.df$score>0, "Positive",ifelse(tweets.df$score<0,"Negative","Neutral"))
tweets.df$polarity <- 0
tweets.df$polarity[tweets.df$sentimentBOW == "Positive"] <- 1
tweets.df$polarity[tweets.df$sentimentBOW == "Negative"] <- -1
tweets.df$dateBOW <- as.Date(tweets.df$created)
result <- aggregate(tweets.df$polarity ~ tweets.df$dateBOW, data = tweets.df, sum)
plot(result, type = "l",main="Sentiment polarity over time", xlab="Date", ylab="Score", pch=18, col="blue")
poswords.sentiment <- nrow(subset(tweets.df, tweets.df$sentimentBOW=="Positive", select=c(sentimentBOW)))
negwords.sentiment <- nrow(subset(tweets.df, tweets.df$sentimentBOW=="Negative", select=c(sentimentBOW)))
neuwords.sentiment <- nrow(subset(tweets.df, tweets.df$sentimentBOW=="Neutral", select=c(sentimentBOW)))

write.csv(tweets.df, file = "tweets_gun_violence.csv",row.names=FALSE)

```
